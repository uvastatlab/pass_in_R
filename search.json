[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Power and Sample Size in R",
    "section": "",
    "text": "Preface\nThis online book contains examples of power and sample size calculations in R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "t_tests.html",
    "href": "t_tests.html",
    "title": "1  T tests",
    "section": "",
    "text": "1.1 Two sample t-test\nFor sample size estimation based on power, we need the following:\nFor power estimation based on sample size, we need the following:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>T tests</span>"
    ]
  },
  {
    "objectID": "t_tests.html#two-sample-t-test",
    "href": "t_tests.html#two-sample-t-test",
    "title": "1  T tests",
    "section": "",
    "text": "hypothesized difference in means (effect size)\npopulation standard deviation\npower of test\nsignificance level of test\ndirection of test: one or two-sided\n\n\n\nhypothesized difference in means (effect size)\npopulation standard deviation\nsample size for each group\nsignificance level of test\ndirection of test: one or two-sided\n\n\n1.1.1 Example: sample size\nWe wish to run an experiment to test if the mean price of what male and female students pay at a library coffee shop is different. Our null hypothesis is no difference. We want to sample enough students to detect a difference of at least 75 cents. We’re not sure which group pays more, so we’ll do a two-sided test. Assume the following:\n\ntrue difference is 0.75 cents\npopulation standard deviation is $2.25.\nwe desire 0.9 power\nour significance level will be 0.05\nthis will be two-sided test\n\nUsing base R (R Core Team 2025):\n\npower.t.test(delta = 0.75, sd = 2.25, sig.level = 0.05, power = 0.9,\n             alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 190.0991\n          delta = 0.75\n             sd = 2.25\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nWe should plan to observe 191 in each group.\nUsing the pwr package (Champely 2020), we must express effect size as Cohen’s d: difference between the means divided by the population (or pooled) standard deviation.\n\nlibrary(pwr)\npwr.t.test(d = 0.75/2.25, sig.level = 0.05, power = 0.9, \n           alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 190.0991\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n1.1.2 Example: power\nWe wish to run an experiment to test if the mean price of what male and female students pay at a library coffee shop is different. Our null hypothesis is no difference. We want to detect a difference of at least 75 cents and we know our sample size will be 60 students per group. We’re not sure which group pays more, so we’ll do a two-sided test. What is the power of our test? Assume the following:\n\ntrue difference is 0.75 cents\npopulation standard deviation is $2.25.\n60 students per group\nour significance level will be 0.05\nthis will be two-sided test\n\nUsing base R (R Core Team 2025):\n\npower.t.test(delta = 0.75, sd = 2.25, sig.level = 0.05, n = 60,\n             alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 60\n          delta = 0.75\n             sd = 2.25\n      sig.level = 0.05\n          power = 0.4407429\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe power of this test will only be about 0.44 if our assumptions are true.\nUsing the pwr package we get the same result:\n\npwr.t.test(d = 0.75/2.25, sig.level = 0.05, n = 60, \n           alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 60\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.4408242\n    alternative = two.sided\n\nNOTE: n is number in *each* group",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>T tests</span>"
    ]
  },
  {
    "objectID": "t_tests.html#paired-t-test",
    "href": "t_tests.html#paired-t-test",
    "title": "1  T tests",
    "section": "1.2 Paired t-test",
    "text": "1.2 Paired t-test\nFor sample size estimation based on power, we need the following:\n\nhypothesized difference between pairs (effect size)\nstandard deviation of differences\npower of test\nsignificance level of test\ndirection of test: one or two-sided\n\nFor power estimation based on sample size, we need the following:\n\nhypothesized difference between pairs (effect size)\nstandard deviation of differences\nsample size\nsignificance level of test\ndirection of test: one or two-sided\n\n\n1.2.1 Example: sample size\nWe wish to run an experiment to see if an ultra-heavy rope-jumping program reduces 40-yard dash times. We will recruit young men ages 18 - 25 and measure their 40-yard dash time in seconds before the program and after. We’ll use a paired t-test to see if the difference in times is greater than 0 (before - after). We want to sample enough subjects to detect a difference of 0.08 seconds. Assume the following:\n\nhypothesized difference of 0.08\nstandard deviation of differences is 0.25\nwe desire 0.9 power\nsignificance level of 0.05\none-sided test since we assume dash times will only get faster\n\nUsing base R:\n\npower.t.test(delta = 0.08, sd = 0.25, sig.level = 0.05, power = 0.9, \n             type = \"paired\", alternative = \"one.sided\")\n\n\n     Paired t test power calculation \n\n              n = 85.00257\n          delta = 0.08\n             sd = 0.25\n      sig.level = 0.05\n          power = 0.9\n    alternative = one.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nWe should plan to sample at least 86 subjects.\nUsing the pwr package, we need to express the effect size as Cohen’s d: difference in pairs divided by the standard deviation of the differences.\n\nlibrary(pwr)\npwr.t.test(power = 0.9, d = 0.08 / 0.25,\n           type = \"paired\", alternative = \"greater\")\n\n\n     Paired t test power calculation \n\n              n = 85.00256\n              d = 0.32\n      sig.level = 0.05\n          power = 0.9\n    alternative = greater\n\nNOTE: n is number of *pairs*\n\n\n\n\n1.2.2 Example: power\nWe wish to run an experiment to see if an ultra-heavy rope-jumping program reduces 40-yard dash times. We will recruit young men ages 18 - 25 and measure their 40-yard dash time in seconds before the program and after. We’ll use a paired t-test to see if the difference in times is greater than 0 (before - after). We want to be able to detect a difference of 0.08 seconds. We will have access to 50 subjects. What is the power of our experiment. Assume the following:\n\nhypothesized difference of 0.08\nstandard deviation of differences is 0.25\n50 subjects\nsignificance level of 0.05\none-sided test since we assume dash times will only get faster\n\nUsing base R:\n\npower.t.test(delta = 0.08, sd = 0.25, sig.level = 0.05, n = 50, \n             type = \"paired\", alternative = \"one.sided\")\n\n\n     Paired t test power calculation \n\n              n = 50\n          delta = 0.08\n             sd = 0.25\n      sig.level = 0.05\n          power = 0.7212189\n    alternative = one.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nThe power of this test is about 0.72 if our assumptions are true.\nUsing the pwr package we get the same result:\n\npwr.t.test(n = 50, d = 0.08 / 0.25,\n           type = \"paired\", alternative = \"greater\")\n\n\n     Paired t test power calculation \n\n              n = 50\n              d = 0.32\n      sig.level = 0.05\n          power = 0.7212189\n    alternative = greater\n\nNOTE: n is number of *pairs*\n\n\n\n\n\n\nChampely, Stephane. 2020. Pwr: Basic Functions for Power Analysis. https://doi.org/10.32614/CRAN.package.pwr.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>T tests</span>"
    ]
  },
  {
    "objectID": "p_tests.html",
    "href": "p_tests.html",
    "title": "2  Proportion tests",
    "section": "",
    "text": "2.1 Two-sample proportion test\nFor sample size estimation based on power, we need the following:\nFor power estimation based on sample size, we need the following:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportion tests</span>"
    ]
  },
  {
    "objectID": "p_tests.html#two-sample-proportion-test",
    "href": "p_tests.html#two-sample-proportion-test",
    "title": "2  Proportion tests",
    "section": "",
    "text": "proportion in one group\nproportion in other group\npower of test\nsignificance level of test\ndirection of test: one or two-sided\n\n\n\nproportion in one group\nproportion in other group\nsample size in each group\nsignificance level of test\ndirection of test: one or two-sided\n\n\n2.1.1 Example: sample size\nWe wish to plan an experiment to test if there is a difference in the proportion of male and female college undergraduate students who floss daily. Our null hypothesis is no difference in the proportion that answer yes. We want to sample enough students to detect a difference of at least 5%. Assume the following:\n\nproportion of one group is 0.30\nproportion of the other group is 0.25\npower of 0.9\nsignificance level of 0.05\ntwo-sided test\n\nUsing base R (R Core Team 2025):\n\npower.prop.test(p1 = 0.30, p2 = 0.25, sig.level = 0.05, power = 0.9, \n                alternative = \"two.sided\")\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 1673.856\n             p1 = 0.3\n             p2 = 0.25\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nWe need to observe 1674 students in each group.\nNotice the sample size decreases if assume proportions are closer to 0 or 1. For example, assume 0.05 in one group and 0.10 for the other.\n\npower.prop.test(p1 = 0.05, p2 = 0.10, sig.level = 0.05, power = 0.9, \n                alternative = \"two.sided\")\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 581.0821\n             p1 = 0.05\n             p2 = 0.1\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nNow we need to observe 582 in each group.\nUsing the pwr package (Champely 2020), we need to express the difference in proportions as an effect size using ES.h():\n\nlibrary(pwr)\npwr.2p.test(h = ES.h(p1 = 0.05, p2 = 0.10), sig.level = 0.05, power = 0.9,\n            alternative = \"two.sided\")\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1924743\n              n = 567.2579\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\nWe need to observe 568 in each group. The result does not match the base R function because they each calculate effect size differently.\n\n\n2.1.2 Example: power\nWe wish to plan an experiment to test if there is a difference in the proportion of male and female college undergraduate students who floss daily. Our null hypothesis is no difference in the proportion that answer yes. We want to detect a difference of at least 5%. What is the power of our experiment if we know in advance we will be able to sample 300 males and females each? Assume the following:\n\nproportion of one group is 0.30\nproportion of the other group is 0.25\nsample size per group of 300\nsignificance level of 0.05\ntwo-sided test\n\nUsing base R (R Core Team 2025):\n\npower.prop.test(p1 = 0.30, p2 = 0.25, sig.level = 0.05, n = 300, \n                alternative = \"two.sided\")\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 300\n             p1 = 0.3\n             p2 = 0.25\n      sig.level = 0.05\n          power = 0.2777839\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe power of this test will only be about 0.28 if our assumptions are true.\nThe pwr.2p.test() function from the pwr package returns the same answer.\n\npwr.2p.test(h = ES.h(p1 = 0.25, p2 = 0.30), sig.level = 0.05, n = 300,\n            alternative = \"two.sided\")\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1120819\n              n = 300\n      sig.level = 0.05\n          power = 0.2789492\n    alternative = two.sided\n\nNOTE: same sample sizes",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportion tests</span>"
    ]
  },
  {
    "objectID": "p_tests.html#difference-in-proportions",
    "href": "p_tests.html#difference-in-proportions",
    "title": "2  Proportion tests",
    "section": "2.2 Difference in proportions",
    "text": "2.2 Difference in proportions\nFor sample size estimation based on precision, we need the following:\n\nproportion in one group\nproportion in other group\ndesired width of confidence interval\n\n\n2.2.1 Example\nWe wish to plan an experiment to test if there is a difference in the proportion of male and female college undergraduate students who floss daily. If there truly is a difference of 0.05 in the population, we would like to estimate it within 0.025. That implies estimating a confidence interval with a width of 0.05. Assume the following:\n\nproportion of one group is 0.30\nproportion of the other group is 0.25\nconfidence interval width of 0.05\n\nUsing the prec_riskdiff() function from the presize package (Haynes et al. 2021), we can calculate this as follows. (The Newcombe method is the default method.)\n\nlibrary(presize)\nprec_riskdiff(p1 = 0.30, p2 = 0.25, conf.width = 0.05, method = \"newcombe\")\n\n\n     sample size for a risk difference with newcombe confidence interval \n\n   p1   p2       n1       n2     ntot r delta        lwr        upr conf.width\n1 0.3 0.25 2441.299 2441.299 4882.598 1  0.05 0.02495861 0.07495861       0.05\n  conf.level\n1       0.95\n\n\nTo estimate a difference in proportions with this precision, assuming the group proportions are each 0.30 and 0.25, we need to sample 2442 subjects in each group.\nOnce again, assumed proportions closer to 0 or 1 require smaller samples:\n\nprec_riskdiff(p1 = 0.05, p2 = 0.10, conf.width = 0.05, method = \"newcombe\")\n\n\n     sample size for a risk difference with newcombe confidence interval \n\n    p1  p2       n1       n2     ntot r delta         lwr         upr\n1 0.05 0.1 860.9835 860.9835 1721.967 1 -0.05 -0.07525428 -0.02525428\n  conf.width conf.level\n1       0.05       0.95\n\n\nLarger confidence widths also require smaller sample sizes. For example, to estimate a difference in proportions within 0.05 (i.e., confidence width of 0.1):\n\nprec_riskdiff(p1 = 0.05, p2 = 0.10, conf.width = 0.1, method = \"newcombe\")\n\n\n     sample size for a risk difference with newcombe confidence interval \n\n    p1  p2       n1       n2     ntot r delta        lwr           upr\n1 0.05 0.1 225.9885 225.9885 451.9769 1 -0.05 -0.1008756 -0.0008756156\n  conf.width conf.level\n1        0.1       0.95\n\n\n\n\n\n\nChampely, Stephane. 2020. Pwr: Basic Functions for Power Analysis. https://doi.org/10.32614/CRAN.package.pwr.\n\n\nHaynes, Alan G., Armando Lenz, Odile Stalder, and Andreas Limacher. 2021. “‘Presize‘: An r-Package for Precision-Based Sample Size Calculation in Clinical Research.” Journal of Open Source Software 6 (60): 3118. https://doi.org/10.21105/joss.03118.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportion tests</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "3  ANOVA",
    "section": "",
    "text": "3.1 One-way ANOVA\nWe will calculate power and sample size for a one-way ANOVA, a two-way ANOVA, and an ANCOVA. We will use base R, and the pwr and powertools packages.\nWe use a one-way ANOVA to calculate power and sample size when we want to get the difference in means across groups that are from a single factor.\nFor power estimation based on sample size we will use:\nImagine we have a book club with three groups that each use a different medium to interact with the book. Group A reads the book, group b listens to the audiobook, and group c watches the movie. We want to know how well each group understood the story line. Each group has 20 people, the expected population group means are 8, 5, and 12 and the populated standard deviation is expected to be 7.\nWe will calculate the power with \\(\\alpha\\) = 0.05.\nUsing base R:\npower.anova.test(groups = 3,      # 3 groups\n                 between.var = 13,# between group variance\n                 within.var = 49, # within group variance\n                 sig.level=0.05,  # sig level \n                 n = 20)          # no. people in each group \n\n\n     Balanced one-way analysis of variance power calculation \n\n         groups = 3\n              n = 20\n    between.var = 13\n     within.var = 49\n      sig.level = 0.05\n          power = 0.8180084\n\nNOTE: n is number in each group\nUsing powertools, we use anova1way.F.bal() to calculate the overall sample size for balanced groups (each group has 20 people)\nanova1way.F.bal(n = 20, mvec = c(8, 5, 12), sd = 7, power = NULL) |&gt; \n  round(2)\n\n[1] 0.8\nUsing pwr, we need to provide the effect size. We will first calculate it by hand:\n# Calculate Cohen's f \nbook.means &lt;- c(8, 5, 12)\nsd   &lt;- 7\n\nk &lt;- length(book.means)\nmu &lt;- mean(book.means)\n\n# Calculate between group var \nbetween.var.pop &lt;- sum((book.means - mu)^2) / k \nwithin.var &lt;- sd^2 # within group var \n\nf &lt;- sqrt(between.var.pop / within.var)\n\nf\n\n[1] 0.4096345\nNow, we use the function es.anova.f() from powertools to calculate f. We get the same result as above\n# Use powertools to get the effect size \nf.pt &lt;- es.anova.f(means = book.means, sd = sd, v = F)\n\nf.pt\n\n[1] 0.4096345\n# Power analysis using f from manual calculation \npwr.anova.test(k = 3, f = f, n = 20, sig.level = 0.05)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 20\n              f = 0.4096345\n      sig.level = 0.05\n          power = 0.796186\n\nNOTE: n is number in each group\n\n# Power analysis using f calculated by powertools\npwr.anova.test(k = 3, f = f.pt, n = 20, sig.level = 0.05)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 20\n              f = 0.4096345\n      sig.level = 0.05\n          power = 0.796186\n\nNOTE: n is number in each group\nFor base R, powertools, and pwr, we see that our power estimate is 0.8.\nNow, we will calculate sample size for a one-way ANOVA when we know the power we want to achieve, but we need to determine the sample size.\nWhat you need:\nUsing base R, we leave n (sample size) empty and input the power we would like to achieve. We will include a power of 0.8, indicating that if a difference actually exists between the mean and target, we have a 80% chance of detecting it.\npower.anova.test(groups = 3, \n                 between.var = 13,\n                 within.var = 49,\n                 sig.level = 0.05,\n                 power = 0.8) # We input the power as 0.8\n\n\n     Balanced one-way analysis of variance power calculation \n\n         groups = 3\n              n = 19.19235\n    between.var = 13\n     within.var = 49\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\nUsing powertools:\nanova1way.F.bal(n = NULL, mvec = c(8, 5, 12), sd = 7, power = 0.8)\n\n[1] 20.17208\nOur estimated sample size is 21 per group.\nUsing pwr and inputting an effect size of 0.2 instead of estimated mean values.\n# Get f using {powertools}\nf.2way &lt;- es.anova.f(means = c(8, 5, 12), sd = 7, v = F)\n\npwr.anova.test(k = 3, # groups\n               f = f.2way, # effect size \n               sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 20.17208\n              f = 0.4096345\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\nOur estimated sample size is about 21.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "anova.html#one-way-anova",
    "href": "anova.html#one-way-anova",
    "title": "3  ANOVA",
    "section": "",
    "text": "Number of groups and their means (or the group effects)\nGroup sample sizes\nPopulation standard deviation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of groups\nGroup means or effect size\nSignificance level\nPower",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "anova.html#two-way-anova",
    "href": "anova.html#two-way-anova",
    "title": "3  ANOVA",
    "section": "3.2 Two-way ANOVA",
    "text": "3.2 Two-way ANOVA\nExample of power and sample size for a two-way ANOVA, which is used when you want to compare the difference in means across two factors\nWhat you need\n\nNumber of factor levels\nGroup means and standard deviation\nSignificance level\nTarget power\n\nIn our example, we want to evaluate how fertilizer and soil type influence plant growth. We have two types of fertilizer and 2 types of soil. Our dependent variable is plant growth.\nBefore setting up the full study, we wanted to make sure it is sufficiently powered so we ran a pilot study and found the mean values. We want to calculate statistical power of 0.9 at a 95% confidence level.\n\n# Generate matrix of mean values \nplant.mat &lt;- matrix(c(15.2, 19.1, 11.9, 13.3), nrow = 2, byrow = TRUE)\n\n# Print matrix to view \nplant.mat\n\n     [,1] [,2]\n[1,] 15.2 19.1\n[2,] 11.9 13.3\n\n\nUsing powertools:\n\nanova2way.F.bal(n = NULL, mmatrix = plant.mat, sd = 3, alpha = 0.05,\npower = 0.9, v = F)\n\n[1] \"NOTE: The 3rd value for f and power or n is for the interaction\"\n\n\n     nA      nB     nAB \n 5.1626 13.9835 61.0110 \n\n\nUsing pwr2 for a balanced two-way ANOVA\n\n# Get Cohen's f effect sizes using powertools\ncd.anova2way &lt;- es.anova.f(means = plant.mat, sd = 3, v = FALSE)\n\nprint(cd.anova2way)\n\n       fA        fB       fAB \n0.7583333 0.4416667 0.2083333 \n\n\nThe effect sizes for factors A, B, and their interaction(fAB) are returned. We will input fA and fB into our sample size calculation\n\npwr.2way(a = 2, b = 2, # n factors \n         alpha = 0.05, # significance level \n         size.A = 10, size.B = 10, # sample size \n         f.A = cd.anova2way[\"fA\"], f.B = cd.anova2way[\"fB\"]) # effect size\n\n\n     Balanced two-way analysis of variance power calculation \n\n              a = 2\n              b = 2\n            n.A = 10\n            n.B = 10\n      sig.level = 0.05\n        power.A = 0.9966198\n        power.B = 0.776444\n          power = 0.776444\n\nNOTE: power is the minimum power among two factors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "anova.html#ancova",
    "href": "anova.html#ancova",
    "title": "3  ANOVA",
    "section": "3.3 ANCOVA",
    "text": "3.3 ANCOVA\nWith an ANCOVA, we extend our one-way ANOVA by including a variable that we want to account for in the model.\nWhat we need:\n\nGroup means and standard deviation\nEstimated \\(R^2\\)\nSignificance level\nPower\n\nWhen looking at story comprehension based on the medium (book, audiobook, movie), we want to account for age. We will use the function anova1way.F.bal() from powertools and input our mean values, standard deviation, \\(R^2\\), the number of covariates, and the power.\n\nanova1way.F.bal(n = NULL, \n                mvec = c(8, 5, 12),\n                sd = 7, \n                Rsq = 0.6^2, #estimated r^2 \n                ncov = 1, #n covariates \n                power = 0.8)\n\n[1] 13.32866",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "4  Regression",
    "section": "",
    "text": "4.1 Multiple Regression",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#overall-f-test",
    "href": "regression.html#overall-f-test",
    "title": "4  Regression",
    "section": "4.2 Overall F test",
    "text": "4.2 Overall F test\nFor sample size estimation based on power, we need the following:\n\nhypothesized effect size (f2 = R2 / (1 − R2))\nnumerator degrees of freedom (u = number of predictors being tested, occasionally represented as ‘p’)\nsignificance level of test\npower of test\n\nFor power estimation based on sample size, we need the following:\n\nhypothesized effect size (f2 = R2 / (1 − R2))\nnumerator degrees of freedom (u = number of predictors being tested, occasionally represented as ‘p’)\ndenominator degrees of freedom (v = n − u − 1, where n is sample size). Hence, n = v + u +1\nsignificance level of test\n\n\n4.2.1 Example: sample size\nSuppose there is an annual 5k running race in a city. We are interested if we can predict an individual’s time to complete the race using two variables: total hours spent training the past three months and average pace per mile during training runs. We can perform a multiple regression to model race completion time as a function of training time and pace. We want to sample enough subjects to detect an effect when our two independent (explanatory) variables explains 45% of variation of the dependent (response) variable, race completion time. Assume the following:\n\nhypothesized effect size of (f2 = .45 / (1-.45)), f2 = 0.82; R2 = .45\nnumerator degrees of freedom u = 2 (two predictors)\nsignificance level = 0.01\ndesired power = 0.75\n\nUsing the pwr package (Champely 2020):\n\nlibrary(pwr)\npwr.f2.test(f2 = 0.82, u = 2, sig.level = 0.01, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 2\n              v = 23.17\n             f2 = 0.82\n      sig.level = 0.01\n          power = 0.9\n\n\nFrom above, we know that n = v + u +1, so we need 24 + 2 + 1 = 27 subjects.\n\n\n4.2.2 Example: power\nSuppose there is an annual 5k running race in a city. We are interested if we can predict an individual’s time to complete the race using two variables: total hours spent training the past three months and average pace per mile during training runs. We can perform a multiple regression to model race completion time as a function of training time and pace. We want to sample enough subjects to detect an effect when our two independent (explanatory) variables explains 45% of variation of the dependent (response) variable, race completion time. We will have 30 participating subjects. Assume the following:\n\nhypothesized effect size of (f2 = .45 / (1-.45)), f2 = 0.82; R2 = .45\nnumerator degrees of freedom u = 2 (two predictors)\ndenominator degrees of freedom v = n − u − 1 = 30 - 2 - 1 = 27\nsignificance level = 0.01\n\nUsing the pwr package [@pwr]:\n\nlibrary(pwr)\npwr.f2.test(f2 = 0.82, u = 2, v = 27, sig.level = 0.01)\n\n\n     Multiple regression power calculation \n\n              u = 2\n              v = 27\n             f2 = 0.82\n      sig.level = 0.01\n          power = 0.9491656\n\n\nThe power is about 0.94.\n\nlibrary(powertools) \nmlrF.overall(N = 30, p = 2, Rsq = 0.45, power = NULL, random = TRUE, v = TRUE)\n\n\n     Power calculation for a multiple linear regression\n     overall F test assuming random predictors \n\n              N = 30\n              p = 2\n            Rsq = 0.45\n            fsq = 0.8181818\n          alpha = 0.05\n          power = 0.983175\n\n\nThis can be done using the powertools package (Crespi and Liu 2025), we use the ‘random’ argument to specify that our variables are random (as opposed to fixed), and the ‘v’ argument for a verbose output:\n\nlibrary(powertools) \nmlrF.overall(N = 30, p = 2, Rsq = 0.45, power = NULL, random = TRUE, v = TRUE)\n\n\n     Power calculation for a multiple linear regression\n     overall F test assuming random predictors \n\n              N = 30\n              p = 2\n            Rsq = 0.45\n            fsq = 0.8181818\n          alpha = 0.05\n          power = 0.983175\n\n\n\n\n4.2.3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#partial-f-tests",
    "href": "regression.html#partial-f-tests",
    "title": "4  Regression",
    "section": "4.3 Partial F Tests",
    "text": "4.3 Partial F Tests",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#logistic-regression",
    "href": "regression.html#logistic-regression",
    "title": "4  Regression",
    "section": "4.4 Logistic Regression",
    "text": "4.4 Logistic Regression",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#poisson-regression",
    "href": "regression.html#poisson-regression",
    "title": "4  Regression",
    "section": "4.5 Poisson Regression",
    "text": "4.5 Poisson Regression\n\n\n\n\nChampely, Stephane. 2020. Pwr: Basic Functions for Power Analysis. https://doi.org/10.32614/CRAN.package.pwr.\n\n\nCrespi, Catherine M., and Zichen Liu. 2025. Powertools: Power and Sample Size Tools. https://doi.org/10.32614/CRAN.package.powertools.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "mixed.html",
    "href": "mixed.html",
    "title": "5  Mixed-effect models",
    "section": "",
    "text": "5.1 Random intercept model: continuous slope\nIn this model the slope coefficient represents a trend.\nFor power estimation based on sample size, we need the following:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effect models</span>"
    ]
  },
  {
    "objectID": "mixed.html#random-intercept-model-continuous-slope",
    "href": "mixed.html#random-intercept-model-continuous-slope",
    "title": "5  Mixed-effect models",
    "section": "",
    "text": "Between group standard deviation (Intercept random effect)\nWithin group standard deviation (residual standard deviation)\nSample size (n) in each group (N)\nEffect size (slope coefficient)\nIntercept coefficient\nSignificance level of test\nDirection of test: one or two-sided\n\n\n5.1.1 Example\nWe are planning an experiment where we track the BMI of college students over their four years at school. We’ll measure their BMI at the beginning of each school year in August. We would like sufficient power to detect a year trend of at least 0.25 if we recruit 15 students to the study. The null hypothesis we’ll test is that the year (slope) coefficient is 0. Rejecting this hypothesis provides evidence of a change in BMI over time. Assume the following:\n\nBetween group standard deviation of 3\nWithin group standard deviation 1\nSample size in each group: 4 obs each of 15 students\nEffect size of 0.25 (slope coefficient)\nIntercept coefficient: 20\nSignificance level of test: 0.01\nDirection of test: two-sided\n\nWe can use simulation to estimate the power of a hypothesis test on a specific coefficient in a model. This example uses the simr package (Green and MacLeod 2016).\nFirst we create a data frame of 4 observations each of 15 students.\n\nlibrary(simr)\nyear &lt;- 1:4\nstudent &lt;- 1:15\nd &lt;- expand.grid(year = year, student = student)\nhead(d, n = 8)\n\n  year student\n1    1       1\n2    2       1\n3    3       1\n4    4       1\n5    1       2\n6    2       2\n7    3       2\n8    4       2\n\n\nNow define our between and within group variance. Note that the simr package requires the between group variance be expressed as variance and the within group variance be expressed as standard deviation.\n\nbetween &lt;- 3^2 # random intercept variance\nwithin &lt;- 1 # residual standard deviation\n\nNext define our effects. We need to provide intercept and slope coefficients. The slope coefficient is the effect we will test. The intercept can be interpreted as the average BMI for the students’ first year of college.\n\nb &lt;- c(20, 0.25) # fixed intercept and slope\n\nNow define our model using the makeLmer() function. This function will simulate BMI values based on our hypothesized coefficients and variances.\n\nm &lt;- makeLmer(bmi ~ year + (1|student), \n              fixef = b, \n              VarCorr = between, \n              sigma = within, \n              data= d)\nprint(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: bmi ~ year + (1 | student)\n   Data: d\nREML criterion at convergence: 243.8129\nRandom effects:\n Groups   Name        Std.Dev.\n student  (Intercept) 3       \n Residual             1       \nNumber of obs: 60, groups:  student, 15\nFixed Effects:\n(Intercept)         year  \n      20.00         0.25  \n\n\nFinally use the powerSim() function to estimate the power of the test on the slope using 100 simulations. Setting progress = FALSE suppresses a progress bar which we don’t want to output in this online book. The seed argument makes the following result reproducible. The fixed() function dictates what coefficient we want to test. In this case we want to test the “year” coefficient using a “t” test.\n\n# power of test on slope\npowerSim(m, \n         test = fixed(\"year\", \"t\"),\n         seed = 1,\n         nsim = 100, \n         alpha = 0.01, \n         progress = FALSE)\n\nPower for predictor 'year', (95% confidence interval):\n      37.00% (27.56, 47.24)\n\nTest: t-test with Satterthwaite degrees of freedom (package lmerTest)\n      Effect size for year is 0.25\n\nBased on 100 simulations, (0 warnings, 0 errors)\nalpha = 0.01, nrow = 60\n\nTime elapsed: 0 h 0 m 7 s\n\n\nOur power is estimated to be about 0.37. The confidence interval is based on a simple binomial test.\n\nround(binom.test(x = 37, n = 100)$conf.int * 100, 2)\n\n[1] 27.56 47.24\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThis power is low. Let’s try increasing the sample size. We can do that with the extend() function. Below we increase the sample size to 30 and re-run the simulation.\n\nm2 &lt;- extend(m, along = \"student\", n = 30)\npowerSim(m2, \n         test = fixed(\"year\", \"t\"),\n         seed = 2,\n         nsim = 100, \n         alpha = 0.01, \n         progress = FALSE)\n\nPower for predictor 'year', (95% confidence interval):\n      64.00% (53.79, 73.36)\n\nTest: t-test with Satterthwaite degrees of freedom (package lmerTest)\n      Effect size for year is 0.25\n\nBased on 100 simulations, (0 warnings, 0 errors)\nalpha = 0.01, nrow = 120\n\nTime elapsed: 0 h 0 m 6 s\n\n\nPower is higher but still too low at 0.64.\nWe can generate a power curve with varying sample sizes using the powerCurve() function. First extend the data set by the maximum number of students we’re willing to entertain and then call the powerCurve() function. Below we specify a maximum of 40 students. Note this can take a while to run for large data sets and complex models. We have lowered the default number of simulations from 1000 to 100. The breaks argument says we want to try sample sizes ranging from 30 to 40 in steps of 2.\n\nm3 &lt;- extend(m, along = \"student\", n = 40)\npc &lt;- powerCurve(m3, \n                 test = fixed(\"year\", \"t\"), \n                 seed = 5,\n                 along = \"student\",\n                 breaks = seq(30,40,2),\n                 nsim = 100, \n                 alpha = 0.01,\n                 progress = FALSE)\nplot(pc)\n\n\n\n\n\n\n\n\nIt appears we need about 38 students to achieve power of at least 0.80.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effect models</span>"
    ]
  },
  {
    "objectID": "mixed.html#random-intercept-and-random-slope-model-continuous-slope",
    "href": "mixed.html#random-intercept-and-random-slope-model-continuous-slope",
    "title": "5  Mixed-effect models",
    "section": "5.2 Random intercept and random slope model: continuous slope",
    "text": "5.2 Random intercept and random slope model: continuous slope\nIn this model the slope coefficient represents a trend.\nFor power estimation based on sample size, we need the following:\n\nIntercept random effect\nSlope random effect\nIntercept and slope random effects covariance\nWithin group standard deviation (residual standard deviation)\nSample size (n) in each group (N)\nEffect size (slope coefficient)\nIntercept coefficient\nSignificance level of test\nDirection of test: one or two-sided\n\n\n5.2.1 Example\nWe are planning an experiment where we track the BMI of college students over their four years at school. We’ll measure their BMI at the beginning of each school year in August. We would like sufficient power to detect a year trend of at least 0.25 if we recruit 15 students to the study. The null hypothesis we’ll test is that the year (slope) coefficient is 0. Rejecting this hypothesis provides evidence of a change in BMI over time. Assume the following:\n\nIntercept random effect of 3\nSlope random effect of 0.2\nIntercept and slope random effects covariance of 0.5\nWithin group standard deviation 1\nSample size in each group: 4 obs each of 15 students\nEffect size of 0.25 (slope coefficient)\nIntercept coefficient: 20\nSignificance level of test: 0.01\nDirection of test: two-sided\n\nWe can use simulation to estimate the power of a hypothesis test on a specific coefficient in a model. This example uses the simr package (Green and MacLeod 2016).\nFirst we create a data frame of 4 observations each of 15 students.\n\nyear &lt;- 1:4\nstudent &lt;- 1:15\nd &lt;- expand.grid(year = year, student = student)\nhead(d, n = 8)\n\n  year student\n1    1       1\n2    2       1\n3    3       1\n4    4       1\n5    1       2\n6    2       2\n7    3       2\n8    4       2\n\n\nNow define our intercept and slope random effects. Note that the simr package requires this be entered as a matrix. We also need to enter the random effects as variances, so we square each entry.\n\nre &lt;- matrix(c(3^2, 0.5^2, 0.5^2, 0.2^2), 2) \nrownames(re) &lt;- colnames(re) &lt;- c(\"intercept\", \"slope\")\nre\n\n          intercept slope\nintercept      9.00  0.25\nslope          0.25  0.04\n\n\nThen we define the residual standard deviation.\n\nresid_sd &lt;- 1 # residual standard deviation\n\nNext define our effects. We need to provide intercept and slope coefficients. The slope coefficient is the effect we will test. The intercept can be interpreted as the average BMI for the students’ first year of college.\n\nb &lt;- c(20, 0.25) # fixed intercept and slope\n\nNow define our model using the makeLmer() function. This function will simulate BMI values based on our hypothesized coefficients and variances.\n\nm &lt;- makeLmer(bmi ~ year + (year|student), \n              fixef = b, \n              VarCorr = re, \n              sigma = resid_sd, \n              data= d)\nprint(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: bmi ~ year + (year | student)\n   Data: d\nREML criterion at convergence: 232.0793\nRandom effects:\n Groups   Name        Std.Dev. Corr\n student  (Intercept) 3.0          \n          year        0.2      0.42\n Residual             1.0          \nNumber of obs: 60, groups:  student, 15\nFixed Effects:\n(Intercept)         year  \n      20.00         0.25  \n\n\nFinally use the powerSim() function to estimate the power of the test on the slope using 100 simulations. Setting progress = FALSE suppresses a progress bar which we don’t want to output in this online book. The seed argument makes the following result reproducible. The fixed() function dictates what coefficient we want to test. In this case we want to test the “year” coefficient using a “t” test.\n\n# power of test on slope\npowerSim(m, \n         test = fixed(\"year\", \"t\"),\n         seed = 1,\n         nsim = 100, \n         alpha = 0.01, \n         progress = FALSE)\n\nPower for predictor 'year', (95% confidence interval):\n      19.00% (11.84, 28.07)\n\nTest: t-test with Satterthwaite degrees of freedom (package lmerTest)\n      Effect size for year is 0.25\n\nBased on 100 simulations, (1 warning, 0 errors)\nalpha = 0.01, nrow = 60\n\nTime elapsed: 0 h 0 m 7 s\n\n\nOur power is estimated to be about 0.19. The confidence interval is based on a simple binomial test.\n\nround(binom.test(x = 19, n = 100)$conf.int * 100, 2)\n\n[1] 11.84 28.07\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThis power is low. Let’s try increasing the sample size. We can do that with the extend() function. Below we increase the sample size to 30 and re-run the simulation.\n\nm2 &lt;- extend(m, along = \"student\", n = 30)\npowerSim(m2, \n         test = fixed(\"year\", \"t\"),\n         seed = 2,\n         nsim = 100, \n         alpha = 0.01, \n         progress = FALSE)\n\nPower for predictor 'year', (95% confidence interval):\n      55.00% (44.73, 64.97)\n\nTest: t-test with Satterthwaite degrees of freedom (package lmerTest)\n      Effect size for year is 0.25\n\nBased on 100 simulations, (1 warning, 0 errors)\nalpha = 0.01, nrow = 120\n\nTime elapsed: 0 h 0 m 7 s\n\n\nPower is higher but still too low at 0.55.\nWe can generate a power curve with varying sample sizes using the powerCurve() function. First extend the data set by the maximum number of students we’re willing to entertain and then call the powerCurve() function. Below we specify a maximum of 50 students. Note this can take a while to run for large data sets and complex models. We have lowered the default number of simulations from 1000 to 100. The breaks argument says we want to try sample sizes ranging from 40 to 50 in steps of 2.\n\nm3 &lt;- extend(m, along = \"student\", n = 50)\npc &lt;- powerCurve(m3, \n                 test = fixed(\"year\", \"t\"), \n                 seed = 5,\n                 along = \"student\",\n                 breaks = seq(40,50,2),\n                 nsim = 100, \n                 alpha = 0.01,\n                 progress = FALSE)\nplot(pc)\n\n\n\n\n\n\n\n\nIt appears we need about 44 students to achieve power of at least 0.80.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effect models</span>"
    ]
  },
  {
    "objectID": "mixed.html#random-intercept-model-binary-slope",
    "href": "mixed.html#random-intercept-model-binary-slope",
    "title": "5  Mixed-effect models",
    "section": "5.3 Random intercept model: binary slope",
    "text": "5.3 Random intercept model: binary slope\nIn this model the slope coefficient represents a population treatment effect. In other words, we’re comparing means between two groups.\nFor power estimation based on sample size, we need the following:\n\nBetween group standard deviation (Intercept random effect)\nWithin group standard deviation (residual standard deviation)\nSample size (n) in each group (N)\nEffect size (slope coefficient)\nIntercept coefficient\nSignificance level of test\nDirection of test: one or two-sided\n\n\n5.3.1 Example: closed-form expression\nWe are planning a multisite experiment where we will compare two types of bug traps, an older model (control) and a newer model (treatment). At each site we will place 5 traps of each type (10 total) and return in 30 days to weigh the amount of invasive bugs captured. We think it would be meaningful if our new trap captured 3 grams of additional bugs on average. We assume there is a between site standard deviation of 2 and a within site standard deviation of 6. How powerful is our experiment if we select 10 sites to run this experiment? Assume a two-sided test and a significance level of 0.05.\nCrespi (2025) describes how to estimate power for a design such as this using a noncentral t distribution. The multisite.cont() function in the powertools package (Crespi and Liu 2025) implements this formula.\nNote the multisite.cont() function requires the total standard deviation of the outcome variable and the intraclass correlation coefficient (ICC) of the random intercept. We can derive both of these from the information given above.\nThe total standard deviation of the outcome is the square root of the sum of the between and within variances:\n\\[\\sqrt{2^2 + 6^2}  = \\sqrt{40}\\]\nThe ICC of the random intercept is the between group variance divided by total variance:\n\\[\\text{ICC} = 4/40 = 0.1 \\]\nThe ICC for the random intercept is provided to the function using the icc0 argument. The total standard deviation of the outcome is provided using the sd argument.\n\nlibrary(powertools)\nmultisite.cont(m = 10,          # number of traps (5 in each condition)\n               J = 10,          # number of sites\n               delta = 3,       # treatment effect\n               sd = sqrt(40),   # total sd of outcome\n               icc0 = 0.1,     # icc of random effect\n               icc1 = 0,        # icc of random slope (0 in this example)\n               alpha = 0.05,    # significance level of test\n               power = NULL,    # set to NULL since we want power\n               v = TRUE)        # verbose, return more detail\n\n\n     Power for test of average treatment effect in multisite trials \n\n         m1, m2 = 5, 5\n              J = 10\n          delta = 3\n             sd = 6.324555\n     icc0, icc1 = 0.1, 0.0\n          alpha = 0.05\n          power = 0.6061384\n          sides = 2\n\nNOTE: m1, m2 are the number of subjects within site in condition 1, condition 2\n      (total of m1 + m2 per site). m1, m2 OR m2, m1 produce equivalent results.\n\n\nThe estimated power is about 0.61.\nWe can try multiple sample sizes using the sapply() function. We apply the multisite.cont() function to sample sizes ranging from 10 - 20. Notice that sample size in this case is the number of sites.\n\npower &lt;- sapply(10:20, function(x)multisite.cont(m = 10,          \n                                 J = x,\n                                 delta = 3,\n                                 sd = sqrt(40),\n                                 icc0 = 0.1,\n                                 icc1 = 0,\n                                 alpha = 0.05,\n                                 power = NULL))\ndata.frame(sample_size = 10:20, power)\n\n   sample_size     power\n1           10 0.6061384\n2           11 0.6574890\n3           12 0.7035244\n4           13 0.7444653\n5           14 0.7806200\n6           15 0.8123479\n7           16 0.8400344\n8           17 0.8640709\n9           18 0.8848413\n10          19 0.9027126\n11          20 0.9180288\n\n\nA sample size of 15 gets us to power over 0.80, and sample size of about 19 gets us up to 0.90.\n\n\n5.3.2 Example: simulation\nWe can also estimate power using simulation via the simr package (Green and MacLeod 2016).\nLet’s restate the setup: we are planning a multisite experiment where we will compare two types of bug traps, an older model (control) and a newer model (treatment). At each site we will place 5 traps of each type (10 total) and return in 30 days to weigh the amount of invasive bugs captured. We think it would be meaningful if our new trap captured 3 grams of additional bugs on average. We assume there is a between site standard deviation of 2 and a within site standard deviation of 6. How powerful is our experiment if we select 10 sites to run this experiment? Assume a two-sided test and a significance level of 0.05.\nFirst we create a data frame of 10 observations each at 10 sites. Then we add a sequence of five zeroes and five ones to indicate treatment. A zero means control and one means treated.\n\nlibrary(simr)\ntrap &lt;- 1:10\nsite &lt;- 1:10\nd &lt;- expand.grid(trap = trap, site = site)\nd$trt &lt;- rep(c(rep(0, 5), rep(1, 5)), 10)\nhead(d, n = 12)\n\n   trap site trt\n1     1    1   0\n2     2    1   0\n3     3    1   0\n4     4    1   0\n5     5    1   0\n6     6    1   1\n7     7    1   1\n8     8    1   1\n9     9    1   1\n10   10    1   1\n11    1    2   0\n12    2    2   0\n\n\nNow define our between and within group variance. Note that the simr package requires the between group variance be expressed as variance and the within group variance be expressed as standard deviation.\n\nbetween &lt;- 2^2 # random intercept variance\nwithin &lt;- 6 # residual standard deviation\n\nNext define our effects. We need to provide intercept and slope coefficients. The slope coefficient is the effect we will test. The intercept can be interpreted as the average mass of bugs captured in the control trap.\n\nb &lt;- c(10, 3) # fixed intercept and slope\n\nNow define our model using the makeLmer() function. This function will simulate mass values based on our hypothesized coefficients and variances.\n\nm &lt;- makeLmer(mass ~ trt + (1|site), \n              fixef = b, \n              VarCorr = between, \n              sigma = within, \n              data = d)\nprint(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: mass ~ trt + (1 | site)\n   Data: d\nREML criterion at convergence: 714.4632\nRandom effects:\n Groups   Name        Std.Dev.\n site     (Intercept) 2       \n Residual             6       \nNumber of obs: 100, groups:  site, 10\nFixed Effects:\n(Intercept)          trt  \n         10            3  \n\n\nFinally use the powerSim() function to estimate the power of the test on the slope using 100 simulations. Setting progress = FALSE suppresses a progress bar which we don’t want to output in this online book. The seed argument makes the following result reproducible. The fixed() function dictates what coefficient we want to test. In this case we want to test the “trt” coefficient using a “t” test.\n\n# power of test on slope\npowerSim(m, \n         test = fixed(\"trt\", \"t\"),\n         seed = 1,\n         nsim = 100, \n         alpha = 0.05, \n         progress = FALSE)\n\nPower for predictor 'trt', (95% confidence interval):\n      65.00% (54.82, 74.27)\n\nTest: t-test with Satterthwaite degrees of freedom (package lmerTest)\n      Effect size for trt is 3.0\n\nBased on 100 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 100\n\nTime elapsed: 0 h 0 m 6 s\n\n\nPower based on simulation is estimated to be about 0.65.\nWe can generate a power curve with varying sample sizes using the powerCurve() function. First extend the data set by the maximum number of sites we’re willing to entertain and then call the powerCurve() function. Below we specify a maximum of 20 sites. Note this can take a while to run for large data sets and complex models. We have lowered the default number of simulations from 1000 to 100. The breaks argument says we want to try sample sizes ranging from 10 to 20 in steps of 1.\n\nm2 &lt;- extend(m, along = \"site\", n = 20)\npc &lt;- powerCurve(m2, \n                 test = fixed(\"trt\", \"t\"), \n                 seed = 5,\n                 along = \"site\",\n                 breaks = 10:20,\n                 nsim = 100, \n                 alpha = 0.05,\n                 progress = FALSE)\nplot(pc)\n\n\n\n\n\n\n\n\nAccording to simulation, we need to sample 16 sites to reliably achieve power of at least 0.80.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effect models</span>"
    ]
  },
  {
    "objectID": "mixed.html#random-intercept-and-random-slope-model-binary-slope",
    "href": "mixed.html#random-intercept-and-random-slope-model-binary-slope",
    "title": "5  Mixed-effect models",
    "section": "5.4 Random intercept and random slope model: binary slope",
    "text": "5.4 Random intercept and random slope model: binary slope\nIn this model the slope coefficient represents a population treatment effect. In other words, we’re comparing means between two groups. The random slope allows the effect of the treatment to depend on the random group.\nFor power estimation based on sample size, we need the following:\n\nIntercept random effect\nSlope random effect\nIntercept and slope random effects covariance (optional)\nWithin group standard deviation (residual standard deviation)\nSample size (n) in each group (N)\nEffect size (slope coefficient)\nIntercept coefficient\nSignificance level of test\nDirection of test: one or two-sided\n\n\n5.4.1 Example: closed-form expression\nWe are planning a multisite experiment where we will compare two types of bug traps, an older model (control) and a newer model (treatment). At each site we will place 5 traps of each type (10 total) and return in 30 days to weigh the amount of invasive bugs captured. We think it would be meaningful if our new trap captured 3 grams of additional bugs on average. We assume there is an Intercept random effect of 2 (between site standard deviation), a slope random effect of \\(\\sqrt{8}\\), and a within site standard deviation of 6. How powerful is our experiment if we select 10 sites to run this experiment? Assume a two-sided test and a significance level of 0.05.\nCrespi (2025) describes how to estimate power for a design such as this using a noncentral t distribution. The multisite.cont() function in the powertools package (Crespi and Liu 2025) implements this formula.\nNote the multisite.cont() function requires the total standard deviation of the outcome variable and two intraclass correlation coefficients (ICC):\n\nicc0: proportion of variance due to random groups\nicc1: proportion of variance due to random slope\n\nThe total standard deviation of the outcome is calculated as follows (Crespi, page 221):\n\\[\\sqrt{Var(Y_{ij})} = \\sqrt{2^2 + \\frac{1}{4}8 + 6^2}  = \\sqrt{42}\\]\nThe ICC of the random intercept is the between group variance divided by total variance:\n\\[\\text{ICC}_0 = 4/42 \\approx 0.095\\]\nThe ICC of the random slope is one-fourth the random slope variance divided by the total variance.\n\\[\\text{ICC}_1 = \\frac{1/4 * 8}{42} \\approx 0.048\\]\nThe ICC for the random intercept is provided to the function using the icc0 argument. The ICC for the random slope is provided using the icc1 argument. The total standard deviation of the outcome is provided using the sd argument.\n\nmultisite.cont(m = 10, J = 10, delta = 3, sd = sqrt(42), \n               icc0 = 0.095,\n               icc1 = 0.048, \n               power = NULL, v = TRUE)\n\n\n     Power for test of average treatment effect in multisite trials \n\n         m1, m2 = 5, 5\n              J = 10\n          delta = 3\n             sd = 6.480741\n     icc0, icc1 = 0.095, 0.048\n          alpha = 0.05\n          power = 0.4319213\n          sides = 2\n\nNOTE: m1, m2 are the number of subjects within site in condition 1, condition 2\n      (total of m1 + m2 per site). m1, m2 OR m2, m1 produce equivalent results.\n\n\nThe estimated power is about 0.43.\nWe can try multiple sample sizes using the sapply() function. We apply the multisite.cont() function to sample sizes ranging from 12 - 30 in increments of 3. Notice that sample size in this case is the number of sites.\n\npower &lt;- sapply(seq(12,30,3), function(x)multisite.cont(m = 10,          \n                                 J = x,\n                                 delta = 3,\n                                 sd = sqrt(42),\n                                 icc0 = 0.095,\n                                 icc1 = 0.048, \n                                 alpha = 0.05,\n                                 power = NULL))\ndata.frame(sample_size = seq(12,30,3), power)\n\n  sample_size     power\n1          12 0.5160467\n2          15 0.6260654\n3          18 0.7162145\n4          21 0.7879091\n5          24 0.8435886\n6          27 0.8859939\n7          30 0.9177628\n\n\nA sample size of 24 gets us to power over 0.80, and sample size of 30 gets us up to 0.90.\n\n\n5.4.2 Example: simulation\nWe can also estimate power using simulation via the simr package.\nLet’s restate the setup: We are planning a multisite experiment where we will compare two types of bug traps, an older model (control) and a newer model (treatment). At each site we will place 5 traps of each type (10 total) and return in 30 days to weigh the amount of invasive bugs captured. We think it would be meaningful if our new trap captured 3 grams of additional bugs on average. We assume there is an Intercept random effect of 2 (between site standard deviation), a slope random effect of \\(\\sqrt{8}\\), and a within site standard deviation of 6. How powerful is our experiment if we select 10 sites to run this experiment? Assume a two-sided test and a significance level of 0.05.\nFirst we create a data frame of 10 observations each at 10 sites. Then we add a sequence of five zeroes and five ones to indicate treatment. A zero means control and one means treated.\n\nsite &lt;- 1:10\ntrap &lt;- 1:10\nd &lt;- expand.grid(trap = trap, site = site)\nd$trt &lt;- rep(c(rep(0, 5), rep(1, 5)), 10)\nhead(d)\n\n  trap site trt\n1    1    1   0\n2    2    1   0\n3    3    1   0\n4    4    1   0\n5    5    1   0\n6    6    1   1\n\n\nNow define our intercept and slope random effects. Note that the simr package requires these be expressed as variances in a matrix. The within group variance needs to be expressed as standard deviation. To replicate the example above using a closed-form expression, we set the covariance of the random effects to 0.\n\nbetween &lt;- matrix(c(4,0,0,8), nrow = 2)\nwithin &lt;- 6 \n\nNext define our effects. We need to provide intercept and slope coefficients. The slope coefficient is the effect we will test. The intercept can be interpreted as the average mass of bugs captured in the control trap.\n\nb &lt;- c(10, 3) # fixed intercept and slope\n\nNow define our model using the makeLmer() function. This function will simulate mass values based on our hypothesized coefficients and variances.\n\nm &lt;- makeLmer(mass ~ trt + (trt|site), \n              fixef = b, \n              VarCorr = between, \n              sigma = within, \n              data= d)\nprint(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: mass ~ trt + (trt | site)\n   Data: d\nREML criterion at convergence: 755.1639\nRandom effects:\n Groups   Name        Std.Dev. Corr\n site     (Intercept) 2.000        \n          trt         2.828    0.00\n Residual             6.000        \nNumber of obs: 100, groups:  site, 10\nFixed Effects:\n(Intercept)          trt  \n         10            3  \n\n\nFinally use the powerSim() function to estimate the power of the test on the slope using 100 simulations. Setting progress = FALSE suppresses a progress bar which we don’t want to output in this online book. The seed argument makes the following result reproducible. The fixed() function dictates what coefficient we want to test. In this case we want to test the “trt” coefficient using a “t” test.\n\npowerSim(m, \n         test = fixed(\"trt\", \"t\"),\n         seed = 2000,\n         nsim = 100, \n         alpha = 0.05, \n         progress = FALSE)\n\nPower for predictor 'trt', (95% confidence interval):\n      41.00% (31.26, 51.29)\n\nTest: t-test with Satterthwaite degrees of freedom (package lmerTest)\n      Effect size for trt is 3.0\n\nBased on 100 simulations, (32 warnings, 0 errors)\nalpha = 0.05, nrow = 100\n\nTime elapsed: 0 h 0 m 7 s\n\n\nPower based on simulation is estimated to be about 0.41.\nWe can generate a power curve with varying sample sizes using the powerCurve() function. First extend the data set by the maximum number of sites we’re willing to entertain and then call the powerCurve() function. Below we specify a maximum of 30 sites. Note this can take a while to run for large data sets and complex models. We have lowered the default number of simulations from 1000 to 100. The breaks argument says we want to try sample sizes ranging from 12 to 30 in steps of 3.\n\nm2 &lt;- extend(m, along = \"site\", n = 30)\npc &lt;- powerCurve(m2, \n                 test = fixed(\"trt\", \"t\"), \n                 seed = 5,\n                 along = \"site\",\n                 breaks = seq(12, 30, 3),\n                 nsim = 100, \n                 alpha = 0.05,\n                 progress = FALSE)\nplot(pc)\n\n\n\n\n\n\n\n\nAccording to simulation, we need to sample 30 sites to reliably achieve power of at least 0.80. We see that the assumption of random slopes requires larger sample sizes to achieve sufficient levels of power.\n\n\n\n\nCrespi, Catherine M. 2025. Power and Sample Size in r. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9780429488788.\n\n\nCrespi, Catherine M., and Zichen Liu. 2025. Powertools: Power and Sample Size Tools. https://doi.org/10.32614/CRAN.package.powertools.\n\n\nGreen, Peter, and Catriona J. MacLeod. 2016. “Simr: An r Package for Power Analysis of Generalised Linear Mixed Models by Simulation.” Methods in Ecology and Evolution 7 (4): 493–98. https://doi.org/10.1111/2041-210X.12504.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effect models</span>"
    ]
  },
  {
    "objectID": "tips.html",
    "href": "tips.html",
    "title": "6  Tips",
    "section": "",
    "text": "6.1 Adjusting for Dropout\nIf you anticipate dropouts or non-response, expressed as a proportion \\(p\\), inflate the sample size as follows (Crespi 2025):\n\\[ \\frac{N}{1 - p}\\]\nFor example, if you estimate a sample size of 300 but anticipate a dropout rate of 0.1, plan on inflating your sample size by 34 subjects to 334.\n300/(1 - 0.1)\n\n[1] 333.3333",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips</span>"
    ]
  },
  {
    "objectID": "tips.html#post-hoc-power",
    "href": "tips.html#post-hoc-power",
    "title": "6  Tips",
    "section": "6.2 Post-Hoc Power",
    "text": "6.2 Post-Hoc Power\nPost-Hoc Power is not really a thing.\n\nIn studies that fail to yield “statistically significant” results, it is common for reviewers, or even editors, to ask the authors to include a post hoc power calculation. In such situations, editors would like to distinguish between true negatives and false negatives (concluding there is no effect, when there actually is an effect, and the study was just too small to pick it up). However, reporting post-hoc power is nothing more than reporting the p-value a different way, and will therefore not answer the question editors want to know.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips</span>"
    ]
  },
  {
    "objectID": "tips.html#sensitivity-analyses",
    "href": "tips.html#sensitivity-analyses",
    "title": "6  Tips",
    "section": "6.3 Sensitivity Analyses",
    "text": "6.3 Sensitivity Analyses\n\n\n\n\nCrespi, Catherine M. 2025. Power and Sample Size in r. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9780429488788.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Champely, Stephane. 2020. Pwr: Basic Functions for Power\nAnalysis. https://doi.org/10.32614/CRAN.package.pwr.\n\n\nCrespi, Catherine M. 2025. Power and Sample Size in r. 1st ed.\nChapman; Hall/CRC. https://doi.org/10.1201/9780429488788.\n\n\nCrespi, Catherine M., and Zichen Liu. 2025. Powertools: Power and\nSample Size Tools. https://doi.org/10.32614/CRAN.package.powertools.\n\n\nGreen, Peter, and Catriona J. MacLeod. 2016. “Simr: An r Package\nfor Power Analysis of Generalised Linear Mixed Models by\nSimulation.” Methods in Ecology and Evolution 7 (4):\n493–98. https://doi.org/10.1111/2041-210X.12504.\n\n\nHaynes, Alan G., Armando Lenz, Odile Stalder, and Andreas Limacher.\n2021. “‘Presize‘: An r-Package for Precision-Based Sample Size\nCalculation in Clinical Research.” Journal of Open Source\nSoftware 6 (60): 3118. https://doi.org/10.21105/joss.03118.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.",
    "crumbs": [
      "References"
    ]
  }
]